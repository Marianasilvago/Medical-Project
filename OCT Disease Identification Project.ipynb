{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RC0-___LGv16",
    "outputId": "2c4e965f-13c2-474a-a953-99f432a8370a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "%tensorflow_version 2.x\n",
    "import tensorflow as tf\n",
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0':\n",
    "  raise SystemError('GPU device not found')\n",
    "print('Found GPU at: {}'.format(device_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 995
    },
    "id": "60AtAcRfHE2i",
    "outputId": "51b6941b-9987-4ced-ae85-07b9927dde34"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==1.15\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/98/5a99af92fb911d7a88a0005ad55005f35b4c1ba8d75fba02df726cd936e6/tensorflow-1.15.0-cp36-cp36m-manylinux2010_x86_64.whl (412.3MB)\n",
      "\u001b[K     |████████████████████████████████| 412.3MB 43kB/s \n",
      "\u001b[?25hCollecting tensorboard<1.16.0,>=1.15.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
      "\u001b[K     |████████████████████████████████| 3.8MB 55.3MB/s \n",
      "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.15.0)\n",
      "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.33.2)\n",
      "Collecting tensorflow-estimator==1.15.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
      "\u001b[K     |████████████████████████████████| 512kB 58.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.8.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.3.0)\n",
      "Collecting keras-applications>=1.0.8\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
      "\u001b[K     |████████████████████████████████| 51kB 9.7MB/s \n",
      "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.35.1)\n",
      "Collecting gast==0.2.2\n",
      "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
      "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.10.0)\n",
      "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (3.12.4)\n",
      "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.1.2)\n",
      "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.18.5)\n",
      "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (0.2.0)\n",
      "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.15) (1.12.1)\n",
      "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (50.3.2)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.3.3)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (1.0.1)\n",
      "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15) (2.10.0)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (2.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15) (3.4.0)\n",
      "Building wheels for collected packages: gast\n",
      "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for gast: filename=gast-0.2.2-cp36-none-any.whl size=7542 sha256=15089acddc3fc27b5a872bf9fc3fd8bae99600f3c938f3b1f9ff6f7acd393567\n",
      "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
      "Successfully built gast\n",
      "\u001b[31mERROR: tensorflow-probability 0.11.0 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
      "Installing collected packages: tensorboard, tensorflow-estimator, keras-applications, gast, tensorflow\n",
      "  Found existing installation: tensorboard 2.3.0\n",
      "    Uninstalling tensorboard-2.3.0:\n",
      "      Successfully uninstalled tensorboard-2.3.0\n",
      "  Found existing installation: tensorflow-estimator 2.3.0\n",
      "    Uninstalling tensorflow-estimator-2.3.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.3.0\n",
      "  Found existing installation: gast 0.3.3\n",
      "    Uninstalling gast-0.3.3:\n",
      "      Successfully uninstalled gast-0.3.3\n",
      "  Found existing installation: tensorflow 2.3.0\n",
      "    Uninstalling tensorflow-2.3.0:\n",
      "      Successfully uninstalled tensorflow-2.3.0\n",
      "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n"
     ]
    },
    {
     "data": {
      "application/vnd.colab-display-data+json": {
       "pip_warning": {
        "packages": [
         "gast",
         "tensorboard",
         "tensorflow"
        ]
       }
      }
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "!pip install tensorflow==1.15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TS_J8TdJHSqy",
    "outputId": "62a1c605-23c7-486c-83c3-578ee611eb93"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keras==1.2\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ef/39/06886076de3d1eaae6df9aace977284a3c1267f8576c3d07ca0d37437b38/Keras-1.2.0.tar.gz (167kB)\n",
      "\r",
      "\u001b[K     |██                              | 10kB 22.4MB/s eta 0:00:01\r",
      "\u001b[K     |████                            | 20kB 28.7MB/s eta 0:00:01\r",
      "\u001b[K     |█████▉                          | 30kB 22.9MB/s eta 0:00:01\r",
      "\u001b[K     |███████▉                        | 40kB 19.0MB/s eta 0:00:01\r",
      "\u001b[K     |█████████▉                      | 51kB 14.0MB/s eta 0:00:01\r",
      "\u001b[K     |███████████▊                    | 61kB 15.2MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████▊                  | 71kB 14.3MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████▊                | 81kB 14.8MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████▋              | 92kB 14.8MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████▋            | 102kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▌          | 112kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▌        | 122kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▌      | 133kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▍    | 143kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▍  | 153kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▍| 163kB 15.1MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 174kB 15.1MB/s \n",
      "\u001b[?25hRequirement already satisfied: theano in /usr/local/lib/python3.6/dist-packages (from keras==1.2) (1.0.5)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==1.2) (3.13)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from keras==1.2) (1.15.0)\n",
      "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.2) (1.18.5)\n",
      "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from theano->keras==1.2) (1.4.1)\n",
      "Building wheels for collected packages: keras\n",
      "  Building wheel for keras (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for keras: filename=Keras-1.2.0-cp36-none-any.whl size=200432 sha256=f54a0c9c9c54b1ecefa273f9d3cb3104f228acd89877b1ca88dd120e5e0cf241\n",
      "  Stored in directory: /root/.cache/pip/wheels/5f/93/d8/053b368a1eccbbc95f62e719932112466958e63eec45a4caae\n",
      "Successfully built keras\n",
      "\u001b[31mERROR: textgenrnn 1.4.1 has requirement keras>=2.1.5, but you'll have keras 1.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: kapre 0.1.3.1 has requirement keras>=2.0.0, but you'll have keras 1.2.0 which is incompatible.\u001b[0m\n",
      "\u001b[31mERROR: fancyimpute 0.4.3 has requirement keras>=2.0.0, but you'll have keras 1.2.0 which is incompatible.\u001b[0m\n",
      "Installing collected packages: keras\n",
      "  Found existing installation: Keras 2.4.3\n",
      "    Uninstalling Keras-2.4.3:\n",
      "      Successfully uninstalled Keras-2.4.3\n",
      "Successfully installed keras-1.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip install keras==1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "sMgKkP84HcEP",
    "outputId": "0b47bab3-79d8-484b-e8f7-a6346894f49e"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.15.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "fFcx3IbwH0SO",
    "outputId": "4cbd9fff-ab1b-4c14-cf29-3ab70ca51a54"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'1.2.0'"
      ]
     },
     "execution_count": 5,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "__2MBgySHgx8",
    "outputId": "f252b013-b7a9-4ca7-9a54-6278c53d22f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DvDHm5JgHlRU"
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "#!pip install opencv-python\n",
    "import os,cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## check dimensions as per theano/tensorflow for image dimensions.\n",
    "from keras import backend as K\n",
    "K.set_image_dim_ordering('th')\n",
    "\n",
    "from keras.layers import Merge,Dense, Conv2D, Flatten, MaxPooling2D, Activation, Dropout, BatchNormalization\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.optimizers import SGD,RMSprop,adam\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ihmZgt-sIDB3",
    "outputId": "cda63bfc-3c5c-4f87-de62-8da2218fa725"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded the images of dataset-NORMAL9\n",
      "\n",
      "Loaded the images of dataset-NORMAL3\n",
      "\n",
      "Loaded the images of dataset-NORMAL15\n",
      "\n",
      "Loaded the images of dataset-NORMAL8\n",
      "\n",
      "Loaded the images of dataset-NORMAL6\n",
      "\n",
      "Loaded the images of dataset-NORMAL4\n",
      "\n",
      "Loaded the images of dataset-NORMAL7\n",
      "\n",
      "Loaded the images of dataset-NORMAL5\n",
      "\n",
      "Loaded the images of dataset-NORMAL2\n",
      "\n",
      "Loaded the images of dataset-NORMAL14\n",
      "\n",
      "Loaded the images of dataset-DME5\n",
      "\n",
      "Loaded the images of dataset-NORMAL10\n",
      "\n",
      "Loaded the images of dataset-DME6\n",
      "\n",
      "Loaded the images of dataset-NORMAL13\n",
      "\n",
      "Loaded the images of dataset-DME9\n",
      "\n",
      "Loaded the images of dataset-NORMAL12\n",
      "\n",
      "Loaded the images of dataset-DME8\n",
      "\n",
      "Loaded the images of dataset-NORMAL11\n",
      "\n",
      "Loaded the images of dataset-DME7\n",
      "\n",
      "Loaded the images of dataset-NORMAL1\n",
      "\n",
      "Loaded the images of dataset-DME14\n",
      "\n",
      "Loaded the images of dataset-DME3\n",
      "\n",
      "Loaded the images of dataset-DME10\n",
      "\n",
      "Loaded the images of dataset-DME13\n",
      "\n",
      "Loaded the images of dataset-DME1\n",
      "\n",
      "Loaded the images of dataset-DME2\n",
      "\n",
      "Loaded the images of dataset-DME15\n",
      "\n",
      "Loaded the images of dataset-DME4\n",
      "\n",
      "Loaded the images of dataset-DME12\n",
      "\n",
      "Loaded the images of dataset-DME11\n",
      "\n",
      "Loaded the images of dataset-AMD6\n",
      "\n",
      "Loaded the images of dataset-AMD2\n",
      "\n",
      "Loaded the images of dataset-AMD5\n",
      "\n",
      "Loaded the images of dataset-AMD8\n",
      "\n",
      "Loaded the images of dataset-AMD14\n",
      "\n",
      "Loaded the images of dataset-AMD3\n",
      "\n",
      "Loaded the images of dataset-AMD4\n",
      "\n",
      "Loaded the images of dataset-AMD7\n",
      "\n",
      "Loaded the images of dataset-AMD15\n",
      "\n",
      "Loaded the images of dataset-AMD9\n",
      "\n",
      "Loaded the images of dataset-AMD10\n",
      "\n",
      "Loaded the images of dataset-AMD12\n",
      "\n",
      "Loaded the images of dataset-AMD1\n",
      "\n",
      "Loaded the images of dataset-AMD13\n",
      "\n",
      "Loaded the images of dataset-AMD11\n",
      "\n"
     ]
    }
   ],
   "source": [
    "PATH = os.getcwd()\n",
    "# Define data path\n",
    "data_path = '/content/drive/MyDrive/Classroom/OCT_Dataset'\n",
    "data_dir_list = os.listdir(data_path)\n",
    "\n",
    "img_rows=128\n",
    "img_cols=128\n",
    "num_channel=1\n",
    "#num_epoch=2\n",
    "no_images=0\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list = os.listdir(data_path + '/' + dataset)\n",
    "    no_images = no_images+len(img_list)\n",
    "\n",
    "# Define the number of classes\n",
    "labels = np.ones((no_images,),dtype='int64')\n",
    "num_classes = 3\n",
    "label_index=0\n",
    "img_data_list=[]\n",
    "img=0\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    img_list=os.listdir(data_path+'/'+ dataset)\n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "    for img in img_list:\n",
    "        input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img)\n",
    "        input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "        input_img_resize=cv2.resize(input_img,(128,128))\n",
    "        img_data_list.append(input_img_resize)\n",
    "        if dataset[0]==  'A':\n",
    "            labels[label_index]=  0\n",
    "            #print(dataset[0])\n",
    "        if dataset[0] == 'D':\n",
    "            labels[label_index] = 1\n",
    "            #print(dataset[0])\n",
    "        if dataset[0] == 'N':\n",
    "            labels[label_index] = 2\n",
    "        label_index = label_index+1\n",
    "            #print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9Vylm6yIIEws",
    "outputId": "cae295f0-4a49-44f3-ccf6-a34ed2114892"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3234, 128, 128)\n",
      "(3234, 1, 128, 128)\n",
      "Loaded the images of dataset-NORMAL9\n",
      "\n",
      "Loaded the images of dataset-NORMAL3\n",
      "\n",
      "Loaded the images of dataset-NORMAL15\n",
      "\n",
      "Loaded the images of dataset-NORMAL8\n",
      "\n",
      "Loaded the images of dataset-NORMAL6\n",
      "\n",
      "Loaded the images of dataset-NORMAL4\n",
      "\n",
      "Loaded the images of dataset-NORMAL7\n",
      "\n",
      "Loaded the images of dataset-NORMAL5\n",
      "\n",
      "Loaded the images of dataset-NORMAL2\n",
      "\n",
      "Loaded the images of dataset-NORMAL14\n",
      "\n",
      "Loaded the images of dataset-DME5\n",
      "\n",
      "Loaded the images of dataset-NORMAL10\n",
      "\n",
      "Loaded the images of dataset-DME6\n",
      "\n",
      "Loaded the images of dataset-NORMAL13\n",
      "\n",
      "Loaded the images of dataset-DME9\n",
      "\n",
      "Loaded the images of dataset-NORMAL12\n",
      "\n",
      "Loaded the images of dataset-DME8\n",
      "\n",
      "Loaded the images of dataset-NORMAL11\n",
      "\n",
      "Loaded the images of dataset-DME7\n",
      "\n",
      "Loaded the images of dataset-NORMAL1\n",
      "\n",
      "Loaded the images of dataset-DME14\n",
      "\n",
      "Loaded the images of dataset-DME3\n",
      "\n",
      "Loaded the images of dataset-DME10\n",
      "\n",
      "Loaded the images of dataset-DME13\n",
      "\n",
      "Loaded the images of dataset-DME1\n",
      "\n",
      "Loaded the images of dataset-DME2\n",
      "\n",
      "Loaded the images of dataset-DME15\n",
      "\n",
      "Loaded the images of dataset-DME4\n",
      "\n",
      "Loaded the images of dataset-DME12\n",
      "\n",
      "Loaded the images of dataset-DME11\n",
      "\n",
      "Loaded the images of dataset-AMD6\n",
      "\n",
      "Loaded the images of dataset-AMD2\n",
      "\n",
      "Loaded the images of dataset-AMD5\n",
      "\n",
      "Loaded the images of dataset-AMD8\n",
      "\n",
      "Loaded the images of dataset-AMD14\n",
      "\n",
      "Loaded the images of dataset-AMD3\n",
      "\n",
      "Loaded the images of dataset-AMD4\n",
      "\n",
      "Loaded the images of dataset-AMD7\n",
      "\n",
      "Loaded the images of dataset-AMD15\n",
      "\n",
      "Loaded the images of dataset-AMD9\n",
      "\n",
      "Loaded the images of dataset-AMD10\n",
      "\n",
      "Loaded the images of dataset-AMD12\n",
      "\n",
      "Loaded the images of dataset-AMD1\n",
      "\n",
      "Loaded the images of dataset-AMD13\n",
      "\n",
      "Loaded the images of dataset-AMD11\n",
      "\n",
      "(3234, 16384)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:173: UserWarning: Numerical issues were encountered when centering the data and might not be solved. Dataset may contain too large values. You may need to prescale your features.\n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3234, 16384)\n",
      "(3234, 1, 128, 128)\n",
      "(3234, 1, 128, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/_data.py:190: UserWarning: Numerical issues were encountered when scaling the data and might not be solved. The standard deviation of the data is probably very close to 0. \n",
      "  warnings.warn(\"Numerical issues were encountered \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 9,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data = np.array(img_data_list)\n",
    "img_data = img_data.astype('float32')\n",
    "img_data /= 255\n",
    "print (img_data.shape)\n",
    "\n",
    "\n",
    "# Using 'th' for the image_dim_ordering we get accuracy >=0.99 . \n",
    "# Using 'tf' for the dim order I get accuracy >= 0.9 but on more epochs\n",
    "if num_channel==1:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data= np.expand_dims(img_data, axis=1)\n",
    "        print (img_data.shape)\n",
    "    else:\n",
    "        img_data= np.expand_dims(img_data, axis=4)\n",
    "        print (img_data.shape)\n",
    "\n",
    "else:\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data=np.rollaxis(img_data,3,1)\n",
    "        print (img_data.shape)\n",
    "\n",
    "# case only followed for sequential images\n",
    "# \n",
    "        labels[0:722] = 0\n",
    "        labels[723:1823] = 1\n",
    "        labels[1824:3231] = 2\n",
    "        \n",
    "        \n",
    "        X_train.shape\n",
    "        \n",
    "\n",
    "        \n",
    "USE_SKLEARN_PREPROCESSING=True\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "    # using sklearn for preprocessing\n",
    "    from sklearn import preprocessing\n",
    "\n",
    "    def image_to_feature_vector(image, size=(128, 128)):\n",
    "        # resize the image to a fixed size, then flatten the image into\n",
    "        # a list of raw pixel intensities\n",
    "        return cv2.resize(image, size).flatten()\n",
    "\n",
    "    img_data_list=[]\n",
    "    for dataset in data_dir_list:\n",
    "        img_list=os.listdir(data_path+'/'+ dataset)\n",
    "        print ('Loaded the images of dataset-'+'{}\\n'.format(dataset))\n",
    "        for img in img_list:\n",
    "            input_img=cv2.imread(data_path + '/'+ dataset + '/'+ img )\n",
    "            input_img=cv2.cvtColor(input_img, cv2.COLOR_BGR2GRAY)\n",
    "            input_img_flatten=image_to_feature_vector(input_img,(128,128))\n",
    "            img_data_list.append(input_img_flatten)\n",
    "\n",
    "    img_data = np.array(img_data_list)\n",
    "    img_data = img_data.astype('float32')\n",
    "    print (img_data.shape)\n",
    "    img_data_scaled = preprocessing.scale(img_data)\n",
    "    print (img_data_scaled.shape)\n",
    "\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "    else:\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "\n",
    "    if K.image_dim_ordering()=='th':\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],num_channel,img_rows,img_cols)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "    else:\n",
    "        img_data_scaled=img_data_scaled.reshape(img_data.shape[0],img_rows,img_cols,num_channel)\n",
    "        print (img_data_scaled.shape)\n",
    "\n",
    "if USE_SKLEARN_PREPROCESSING:\n",
    "    img_data=img_data_scaled\n",
    "    \n",
    "    #%%\n",
    "labels[0:1000]\n",
    "#%%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wpYQM-5hZM2F",
    "outputId": "c05ac1c2-d4c7-412f-b80e-7653d3f73bea"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (2587, 1, 128, 128)\n",
      "X_test shape:  (647, 1, 128, 128)\n",
      "y_train shape:  (2587, 3)\n",
      "y_test shape:  (647, 3)\n"
     ]
    }
   ],
   "source": [
    "# Assigning Labels\n",
    "\n",
    "# Define the number of classes\n",
    "num_classes = 3\n",
    "\n",
    "names = ['AMD','DME','NORMAL']\n",
    "\n",
    "# convert class labels to on-hot encoding\n",
    "Y = np_utils.to_categorical(labels, num_classes)\n",
    "\n",
    "#Shuffle the dataset with random state=2\n",
    "x, y = shuffle(img_data, Y, random_state=2)\n",
    "# Split the dataset with 20% testing data\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.2)\n",
    "print(\"X_train shape: \",X_train.shape)\n",
    "print(\"X_test shape: \",X_test.shape)\n",
    "print(\"y_train shape: \",y_train.shape)\n",
    "print(\"y_test shape: \",y_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gVI2YTOuZSBN",
    "outputId": "ad906b31-572a-43a8-ff4a-cc7c147d9638"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_5 (Convolution2D)  (None, 16, 126, 126)  160         convolution2d_input_5[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_5 (MaxPooling2D)    (None, 16, 42, 42)    0           convolution2d_5[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_6 (Convolution2D)  (None, 32, 40, 40)    4640        maxpooling2d_5[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_6 (MaxPooling2D)    (None, 32, 13, 13)    0           convolution2d_6[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_7 (Convolution2D)  (None, 32, 11, 11)    9248        maxpooling2d_6[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_7 (MaxPooling2D)    (None, 32, 3, 3)      0           convolution2d_7[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_5 (Flatten)              (None, 288)           0           maxpooling2d_7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_9 (Dense)                  (None, 16)            4624        flatten_5[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 3)             51          dense_9[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 18,723\n",
      "Trainable params: 18,723\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining the model \n",
    "# Feel free to use CNNs/Dense Networks\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, nb_row=3, nb_col=3, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(3,3)))\n",
    "\n",
    "model.add(Conv2D(32, nb_row=3, nb_col=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(3,3)))\n",
    "\n",
    "model.add(Conv2D(32,nb_row=3, nb_col=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(3,3)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "\n",
    "# Viewing model_configuration\n",
    "model.summary()\n",
    "#model.get_config()\n",
    "#model.layers[0].get_config()\n",
    "#model.layers[0].input_shape\n",
    "#model.layers[0].output_shape\n",
    "#model.layers[0].output\n",
    "#model.layers[0].get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ogJ_znSEZeAM",
    "outputId": "87f9f264-1f72-45b4-e278-6892447cce3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:610: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1966: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1970: The name tf.log is deprecated. Please use tf.math.log instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from keras.optimizers import Adam\n",
    "learning_rate = 0.0001\n",
    "opt = Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_WqLGwZrZkq5",
    "outputId": "677fe2b2-777c-4bad-c0f7-4d5d85fc9c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2587 samples, validate on 647 samples\n",
      "Epoch 1/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.3990 - acc: 0.8821 - val_loss: 0.3871 - val_acc: 0.8794\n",
      "Epoch 2/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.3542 - acc: 0.8976 - val_loss: 0.3492 - val_acc: 0.8918\n",
      "Epoch 3/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.3062 - acc: 0.9181 - val_loss: 0.2947 - val_acc: 0.9150\n",
      "Epoch 4/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.2583 - acc: 0.9331 - val_loss: 0.2575 - val_acc: 0.9258\n",
      "Epoch 5/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.2199 - acc: 0.9436 - val_loss: 0.2103 - val_acc: 0.9490\n",
      "Epoch 6/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.1908 - acc: 0.9548 - val_loss: 0.1872 - val_acc: 0.9521\n",
      "Epoch 7/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.1625 - acc: 0.9617 - val_loss: 0.1713 - val_acc: 0.9598\n",
      "Epoch 8/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.1441 - acc: 0.9691 - val_loss: 0.1493 - val_acc: 0.9645\n",
      "Epoch 9/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.1254 - acc: 0.9749 - val_loss: 0.1411 - val_acc: 0.9675\n",
      "Epoch 10/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.1089 - acc: 0.9814 - val_loss: 0.1258 - val_acc: 0.9753\n",
      "Epoch 11/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0979 - acc: 0.9861 - val_loss: 0.1057 - val_acc: 0.9737\n",
      "Epoch 12/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0866 - acc: 0.9869 - val_loss: 0.1032 - val_acc: 0.9799\n",
      "Epoch 13/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0807 - acc: 0.9872 - val_loss: 0.0967 - val_acc: 0.9815\n",
      "Epoch 14/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0738 - acc: 0.9903 - val_loss: 0.0895 - val_acc: 0.9784\n",
      "Epoch 15/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0680 - acc: 0.9927 - val_loss: 0.0949 - val_acc: 0.9722\n",
      "Epoch 16/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0637 - acc: 0.9927 - val_loss: 0.0785 - val_acc: 0.9784\n",
      "Epoch 17/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0586 - acc: 0.9942 - val_loss: 0.0821 - val_acc: 0.9753\n",
      "Epoch 18/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0550 - acc: 0.9954 - val_loss: 0.0762 - val_acc: 0.9784\n",
      "Epoch 19/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0530 - acc: 0.9942 - val_loss: 0.0725 - val_acc: 0.9784\n",
      "Epoch 20/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0499 - acc: 0.9961 - val_loss: 0.0733 - val_acc: 0.9784\n",
      "Epoch 21/150\n",
      "2587/2587 [==============================] - 0s - loss: 0.0473 - acc: 0.9954 - val_loss: 0.0707 - val_acc: 0.9799\n",
      "Epoch 22/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0458 - acc: 0.9981 - val_loss: 0.0684 - val_acc: 0.9815\n",
      "Epoch 23/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0434 - acc: 0.9965 - val_loss: 0.0688 - val_acc: 0.9784\n",
      "Epoch 24/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0420 - acc: 0.9965 - val_loss: 0.0675 - val_acc: 0.9799\n",
      "Epoch 25/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0410 - acc: 0.9977 - val_loss: 0.0674 - val_acc: 0.9753\n",
      "Epoch 26/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0395 - acc: 0.9981 - val_loss: 0.0642 - val_acc: 0.9815\n",
      "Epoch 27/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0384 - acc: 0.9988 - val_loss: 0.0666 - val_acc: 0.9768\n",
      "Epoch 28/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0366 - acc: 0.9965 - val_loss: 0.0621 - val_acc: 0.9861\n",
      "Epoch 29/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0360 - acc: 0.9992 - val_loss: 0.0641 - val_acc: 0.9784\n",
      "Epoch 30/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0349 - acc: 0.9977 - val_loss: 0.0614 - val_acc: 0.9815\n",
      "Epoch 31/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0342 - acc: 0.9969 - val_loss: 0.0598 - val_acc: 0.9830\n",
      "Epoch 32/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0322 - acc: 0.9985 - val_loss: 0.0598 - val_acc: 0.9830\n",
      "Epoch 33/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0317 - acc: 0.9981 - val_loss: 0.0623 - val_acc: 0.9784\n",
      "Epoch 34/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0313 - acc: 0.9985 - val_loss: 0.0590 - val_acc: 0.9815\n",
      "Epoch 35/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0304 - acc: 0.9981 - val_loss: 0.0582 - val_acc: 0.9830\n",
      "Epoch 36/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0295 - acc: 0.9977 - val_loss: 0.0588 - val_acc: 0.9861\n",
      "Epoch 37/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0292 - acc: 0.9977 - val_loss: 0.0591 - val_acc: 0.9815\n",
      "Epoch 38/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0285 - acc: 0.9992 - val_loss: 0.0605 - val_acc: 0.9830\n",
      "Epoch 39/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0274 - acc: 0.9985 - val_loss: 0.0635 - val_acc: 0.9815\n",
      "Epoch 40/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0267 - acc: 0.9981 - val_loss: 0.0569 - val_acc: 0.9845\n",
      "Epoch 41/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0256 - acc: 0.9988 - val_loss: 0.0584 - val_acc: 0.9830\n",
      "Epoch 42/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0249 - acc: 0.9981 - val_loss: 0.0579 - val_acc: 0.9830\n",
      "Epoch 43/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0236 - acc: 0.9992 - val_loss: 0.0581 - val_acc: 0.9830\n",
      "Epoch 44/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0212 - acc: 0.9985 - val_loss: 0.0560 - val_acc: 0.9830\n",
      "Epoch 45/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0172 - acc: 0.9977 - val_loss: 0.0529 - val_acc: 0.9845\n",
      "Epoch 46/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0139 - acc: 0.9981 - val_loss: 0.0786 - val_acc: 0.9815\n",
      "Epoch 47/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0071 - acc: 0.9996 - val_loss: 0.0731 - val_acc: 0.9861\n",
      "Epoch 48/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0056 - acc: 0.9996 - val_loss: 0.0808 - val_acc: 0.9830\n",
      "Epoch 49/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0053 - acc: 0.9996 - val_loss: 0.0737 - val_acc: 0.9876\n",
      "Epoch 50/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0041 - acc: 1.0000 - val_loss: 0.0723 - val_acc: 0.9845\n",
      "Epoch 51/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0039 - acc: 0.9996 - val_loss: 0.0726 - val_acc: 0.9845\n",
      "Epoch 52/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0032 - acc: 0.9996 - val_loss: 0.0720 - val_acc: 0.9845\n",
      "Epoch 53/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0028 - acc: 1.0000 - val_loss: 0.0688 - val_acc: 0.9845\n",
      "Epoch 54/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0027 - acc: 0.9996 - val_loss: 0.0660 - val_acc: 0.9876\n",
      "Epoch 55/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0024 - acc: 1.0000 - val_loss: 0.0685 - val_acc: 0.9845\n",
      "Epoch 56/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0022 - acc: 1.0000 - val_loss: 0.0668 - val_acc: 0.9861\n",
      "Epoch 57/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0021 - acc: 0.9996 - val_loss: 0.0668 - val_acc: 0.9845\n",
      "Epoch 58/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0019 - acc: 1.0000 - val_loss: 0.0666 - val_acc: 0.9845\n",
      "Epoch 59/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0018 - acc: 1.0000 - val_loss: 0.0685 - val_acc: 0.9845\n",
      "Epoch 60/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0018 - acc: 0.9996 - val_loss: 0.0662 - val_acc: 0.9845\n",
      "Epoch 61/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0668 - val_acc: 0.9845\n",
      "Epoch 62/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0015 - acc: 1.0000 - val_loss: 0.0662 - val_acc: 0.9845\n",
      "Epoch 63/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0014 - acc: 1.0000 - val_loss: 0.0662 - val_acc: 0.9845\n",
      "Epoch 64/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0014 - acc: 0.9996 - val_loss: 0.0648 - val_acc: 0.9861\n",
      "Epoch 65/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0635 - val_acc: 0.9876\n",
      "Epoch 66/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0013 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9876\n",
      "Epoch 67/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0645 - val_acc: 0.9861\n",
      "Epoch 68/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0012 - acc: 0.9996 - val_loss: 0.0637 - val_acc: 0.9876\n",
      "Epoch 69/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0012 - acc: 1.0000 - val_loss: 0.0650 - val_acc: 0.9861\n",
      "Epoch 70/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0011 - acc: 1.0000 - val_loss: 0.0624 - val_acc: 0.9892\n",
      "Epoch 71/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0010 - acc: 1.0000 - val_loss: 0.0617 - val_acc: 0.9892\n",
      "Epoch 72/150\n",
      "2587/2587 [==============================] - 1s - loss: 9.6495e-04 - acc: 1.0000 - val_loss: 0.0623 - val_acc: 0.9845\n",
      "Epoch 73/150\n",
      "2587/2587 [==============================] - 1s - loss: 9.4015e-04 - acc: 1.0000 - val_loss: 0.0644 - val_acc: 0.9845\n",
      "Epoch 74/150\n",
      "2587/2587 [==============================] - 1s - loss: 9.4128e-04 - acc: 1.0000 - val_loss: 0.0625 - val_acc: 0.9861\n",
      "Epoch 75/150\n",
      "2587/2587 [==============================] - 1s - loss: 8.6345e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9861\n",
      "Epoch 76/150\n",
      "2587/2587 [==============================] - 1s - loss: 8.3847e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9861\n",
      "Epoch 77/150\n",
      "2587/2587 [==============================] - 1s - loss: 8.1001e-04 - acc: 1.0000 - val_loss: 0.0616 - val_acc: 0.9876\n",
      "Epoch 78/150\n",
      "2587/2587 [==============================] - 1s - loss: 7.8426e-04 - acc: 1.0000 - val_loss: 0.0612 - val_acc: 0.9861\n",
      "Epoch 79/150\n",
      "2587/2587 [==============================] - 1s - loss: 7.4237e-04 - acc: 1.0000 - val_loss: 0.0621 - val_acc: 0.9861\n",
      "Epoch 80/150\n",
      "2587/2587 [==============================] - 1s - loss: 7.3145e-04 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9892\n",
      "Epoch 81/150\n",
      "2587/2587 [==============================] - 1s - loss: 7.0210e-04 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9861\n",
      "Epoch 82/150\n",
      "2587/2587 [==============================] - 1s - loss: 7.7700e-04 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9876\n",
      "Epoch 83/150\n",
      "2587/2587 [==============================] - 1s - loss: 0.0011 - acc: 0.9996 - val_loss: 0.0671 - val_acc: 0.9876\n",
      "Epoch 84/150\n",
      "2587/2587 [==============================] - 1s - loss: 8.6983e-04 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9892\n",
      "Epoch 85/150\n",
      "2587/2587 [==============================] - 1s - loss: 6.6925e-04 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 0.9892\n",
      "Epoch 86/150\n",
      "2587/2587 [==============================] - 1s - loss: 6.4442e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9876\n",
      "Epoch 87/150\n",
      "2587/2587 [==============================] - 1s - loss: 6.0729e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9876\n",
      "Epoch 88/150\n",
      "2587/2587 [==============================] - 1s - loss: 6.0013e-04 - acc: 1.0000 - val_loss: 0.0584 - val_acc: 0.9892\n",
      "Epoch 89/150\n",
      "2587/2587 [==============================] - 1s - loss: 5.8723e-04 - acc: 1.0000 - val_loss: 0.0616 - val_acc: 0.9845\n",
      "Epoch 90/150\n",
      "2587/2587 [==============================] - 1s - loss: 5.7225e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9892\n",
      "Epoch 91/150\n",
      "2587/2587 [==============================] - 1s - loss: 5.5071e-04 - acc: 1.0000 - val_loss: 0.0598 - val_acc: 0.9861\n",
      "Epoch 92/150\n",
      "2587/2587 [==============================] - 1s - loss: 5.3717e-04 - acc: 1.0000 - val_loss: 0.0596 - val_acc: 0.9892\n",
      "Epoch 93/150\n",
      "2587/2587 [==============================] - 1s - loss: 5.3645e-04 - acc: 1.0000 - val_loss: 0.0605 - val_acc: 0.9892\n",
      "Epoch 94/150\n",
      "2587/2587 [==============================] - 1s - loss: 7.1861e-04 - acc: 0.9996 - val_loss: 0.0568 - val_acc: 0.9892\n",
      "Epoch 95/150\n",
      "2587/2587 [==============================] - 1s - loss: 7.6748e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9907\n",
      "Epoch 96/150\n",
      "2587/2587 [==============================] - 1s - loss: 8.2432e-04 - acc: 0.9996 - val_loss: 0.0667 - val_acc: 0.9861\n",
      "Epoch 97/150\n",
      "2587/2587 [==============================] - 1s - loss: 8.9517e-04 - acc: 1.0000 - val_loss: 0.0673 - val_acc: 0.9861\n",
      "Epoch 98/150\n",
      "2587/2587 [==============================] - 1s - loss: 5.9903e-04 - acc: 1.0000 - val_loss: 0.0589 - val_acc: 0.9892\n",
      "Epoch 99/150\n",
      "2587/2587 [==============================] - 1s - loss: 5.2593e-04 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 0.9876\n",
      "Epoch 100/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.9793e-04 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9892\n",
      "Epoch 101/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.8566e-04 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9876\n",
      "Epoch 102/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.7828e-04 - acc: 1.0000 - val_loss: 0.0552 - val_acc: 0.9907\n",
      "Epoch 103/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.6871e-04 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9876\n",
      "Epoch 104/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.5952e-04 - acc: 1.0000 - val_loss: 0.0557 - val_acc: 0.9892\n",
      "Epoch 105/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.5268e-04 - acc: 1.0000 - val_loss: 0.0561 - val_acc: 0.9892\n",
      "Epoch 106/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.4440e-04 - acc: 1.0000 - val_loss: 0.0558 - val_acc: 0.9892\n",
      "Epoch 107/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.4138e-04 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9892\n",
      "Epoch 108/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.3952e-04 - acc: 1.0000 - val_loss: 0.0562 - val_acc: 0.9892\n",
      "Epoch 109/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.3817e-04 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 0.9876\n",
      "Epoch 110/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.2937e-04 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9892\n",
      "Epoch 111/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.2415e-04 - acc: 1.0000 - val_loss: 0.0566 - val_acc: 0.9892\n",
      "Epoch 112/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.1997e-04 - acc: 1.0000 - val_loss: 0.0570 - val_acc: 0.9876\n",
      "Epoch 113/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.1615e-04 - acc: 1.0000 - val_loss: 0.0568 - val_acc: 0.9892\n",
      "Epoch 114/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.1484e-04 - acc: 1.0000 - val_loss: 0.0571 - val_acc: 0.9892\n",
      "Epoch 115/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.0857e-04 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9892\n",
      "Epoch 116/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.0420e-04 - acc: 1.0000 - val_loss: 0.0576 - val_acc: 0.9892\n",
      "Epoch 117/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.0060e-04 - acc: 1.0000 - val_loss: 0.0569 - val_acc: 0.9892\n",
      "Epoch 118/150\n",
      "2587/2587 [==============================] - 1s - loss: 4.0007e-04 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 0.9892\n",
      "Epoch 119/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.9609e-04 - acc: 1.0000 - val_loss: 0.0572 - val_acc: 0.9892\n",
      "Epoch 120/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.9237e-04 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 0.9892\n",
      "Epoch 121/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.8898e-04 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9892\n",
      "Epoch 122/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.8689e-04 - acc: 1.0000 - val_loss: 0.0578 - val_acc: 0.9892\n",
      "Epoch 123/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.8603e-04 - acc: 1.0000 - val_loss: 0.0580 - val_acc: 0.9892\n",
      "Epoch 124/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.8340e-04 - acc: 1.0000 - val_loss: 0.0582 - val_acc: 0.9892\n",
      "Epoch 125/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.7989e-04 - acc: 1.0000 - val_loss: 0.0581 - val_acc: 0.9892\n",
      "Epoch 126/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.7970e-04 - acc: 1.0000 - val_loss: 0.0573 - val_acc: 0.9892\n",
      "Epoch 127/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.7627e-04 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9892\n",
      "Epoch 128/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.7421e-04 - acc: 1.0000 - val_loss: 0.0585 - val_acc: 0.9892\n",
      "Epoch 129/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.7346e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9892\n",
      "Epoch 130/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.7218e-04 - acc: 1.0000 - val_loss: 0.0575 - val_acc: 0.9892\n",
      "Epoch 131/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.6835e-04 - acc: 1.0000 - val_loss: 0.0587 - val_acc: 0.9892\n",
      "Epoch 132/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.6848e-04 - acc: 1.0000 - val_loss: 0.0586 - val_acc: 0.9892\n",
      "Epoch 133/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.6547e-04 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9892\n",
      "Epoch 134/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.6408e-04 - acc: 1.0000 - val_loss: 0.0579 - val_acc: 0.9892\n",
      "Epoch 135/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.6290e-04 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9892\n",
      "Epoch 136/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5995e-04 - acc: 1.0000 - val_loss: 0.0603 - val_acc: 0.9892\n",
      "Epoch 137/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5845e-04 - acc: 1.0000 - val_loss: 0.0594 - val_acc: 0.9892\n",
      "Epoch 138/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5743e-04 - acc: 1.0000 - val_loss: 0.0599 - val_acc: 0.9892\n",
      "Epoch 139/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5593e-04 - acc: 1.0000 - val_loss: 0.0597 - val_acc: 0.9892\n",
      "Epoch 140/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5422e-04 - acc: 1.0000 - val_loss: 0.0610 - val_acc: 0.9892\n",
      "Epoch 141/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5366e-04 - acc: 1.0000 - val_loss: 0.0602 - val_acc: 0.9892\n",
      "Epoch 142/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5199e-04 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 0.9892\n",
      "Epoch 143/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.5092e-04 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9892\n",
      "Epoch 144/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.4912e-04 - acc: 1.0000 - val_loss: 0.0609 - val_acc: 0.9892\n",
      "Epoch 145/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.4817e-04 - acc: 1.0000 - val_loss: 0.0619 - val_acc: 0.9892\n",
      "Epoch 146/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.4692e-04 - acc: 1.0000 - val_loss: 0.0613 - val_acc: 0.9892\n",
      "Epoch 147/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.4636e-04 - acc: 1.0000 - val_loss: 0.0622 - val_acc: 0.9892\n",
      "Epoch 148/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.4480e-04 - acc: 1.0000 - val_loss: 0.0618 - val_acc: 0.9892\n",
      "Epoch 149/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.4385e-04 - acc: 1.0000 - val_loss: 0.0627 - val_acc: 0.9892\n",
      "Epoch 150/150\n",
      "2587/2587 [==============================] - 1s - loss: 3.4318e-04 - acc: 1.0000 - val_loss: 0.0632 - val_acc: 0.9892\n",
      "640/647 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, nb_epoch=150, batch_size=32, verbose=1, validation_data=(X_test, y_test))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "GfRwmdY5btld",
    "outputId": "d129a740-d678-4c34-9d79-2b30061fc0cf"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy on training set is 1.0\n",
      "\n",
      "Accuracy on test set is 0.9891808346213292\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.squeeze(history.history[\"loss\"]))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\nAccuracy on training set is {}\".format(history.history[\"acc\"][-1]))\n",
    "print(\"\\nAccuracy on test set is {}\".format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JohihGJIa-GB",
    "outputId": "9b5eeef2-ee4c-451b-dd40-062fba80df0a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_8 (Convolution2D)  (None, 16, 126, 126)  160         convolution2d_input_6[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_9 (BatchNorma (None, 16, 126, 126)  504         convolution2d_8[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_8 (MaxPooling2D)    (None, 16, 42, 42)    0           batchnormalization_9[0][0]       \n",
      "____________________________________________________________________________________________________\n",
      "dropout_9 (Dropout)              (None, 16, 42, 42)    0           maxpooling2d_8[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_9 (Convolution2D)  (None, 32, 40, 40)    4640        dropout_9[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_10 (BatchNorm (None, 32, 40, 40)    160         convolution2d_9[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_9 (MaxPooling2D)    (None, 32, 13, 13)    0           batchnormalization_10[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_10 (Dropout)             (None, 32, 13, 13)    0           maxpooling2d_9[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_10 (Convolution2D) (None, 64, 11, 11)    18496       dropout_10[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_11 (BatchNorm (None, 64, 11, 11)    44          convolution2d_10[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_10 (MaxPooling2D)   (None, 64, 3, 3)      0           batchnormalization_11[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dropout_11 (Dropout)             (None, 64, 3, 3)      0           maxpooling2d_10[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "flatten_6 (Flatten)              (None, 576)           0           dropout_11[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_11 (Dense)                 (None, 64)            36928       flatten_6[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "batchnormalization_12 (BatchNorm (None, 64)            256         dense_11[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dropout_12 (Dropout)             (None, 64)            0           batchnormalization_12[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "dense_12 (Dense)                 (None, 3)             195         dropout_12[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 61,383\n",
      "Trainable params: 60,901\n",
      "Non-trainable params: 482\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining the model \n",
    "# Feel free to use CNNs/Dense Networks\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(16, nb_row=3, nb_col=3, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(3,3)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(32, nb_row=3, nb_col=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(3,3)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Conv2D(64,nb_row=3, nb_col=3, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(3,3), strides=(3,3)))\n",
    "model.add(Dropout(0.4))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Viewing model_configuration\n",
    "model.summary()\n",
    "#model.get_config()\n",
    "#model.layers[0].get_config()\n",
    "#model.layers[0].input_shape\n",
    "#model.layers[0].output_shape\n",
    "#model.layers[0].output\n",
    "#model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l6jGg_RtcqNe"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "learning_rate = 0.0001\n",
    "opt = Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_c3rKpcecxag",
    "outputId": "108b25c4-d229-4515-907b-b9a866a23116"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2587 samples, validate on 647 samples\n",
      "Epoch 1/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.6896 - acc: 0.7178 - val_loss: 0.6170 - val_acc: 0.6940\n",
      "Epoch 2/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.6404 - acc: 0.7267 - val_loss: 0.6328 - val_acc: 0.6677\n",
      "Epoch 3/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.6800 - acc: 0.7221 - val_loss: 0.6308 - val_acc: 0.6692\n",
      "Epoch 4/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.6524 - acc: 0.7240 - val_loss: 0.5938 - val_acc: 0.6924\n",
      "Epoch 5/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.6189 - acc: 0.7341 - val_loss: 0.6134 - val_acc: 0.6708\n",
      "Epoch 6/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.6181 - acc: 0.7371 - val_loss: 0.5709 - val_acc: 0.7110\n",
      "Epoch 7/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.6002 - acc: 0.7453 - val_loss: 0.5749 - val_acc: 0.7002\n",
      "Epoch 8/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5575 - acc: 0.7638 - val_loss: 0.5691 - val_acc: 0.7032\n",
      "Epoch 9/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5559 - acc: 0.7634 - val_loss: 0.5616 - val_acc: 0.7110\n",
      "Epoch 10/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5563 - acc: 0.7553 - val_loss: 0.5586 - val_acc: 0.7032\n",
      "Epoch 11/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5658 - acc: 0.7654 - val_loss: 0.5353 - val_acc: 0.7233\n",
      "Epoch 12/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5673 - acc: 0.7646 - val_loss: 0.5287 - val_acc: 0.7280\n",
      "Epoch 13/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5418 - acc: 0.7654 - val_loss: 0.5251 - val_acc: 0.7295\n",
      "Epoch 14/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5330 - acc: 0.7580 - val_loss: 0.5175 - val_acc: 0.7326\n",
      "Epoch 15/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5188 - acc: 0.7812 - val_loss: 0.5090 - val_acc: 0.7403\n",
      "Epoch 16/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.5286 - acc: 0.7812 - val_loss: 0.5005 - val_acc: 0.7372\n",
      "Epoch 17/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4889 - acc: 0.7851 - val_loss: 0.4796 - val_acc: 0.7512\n",
      "Epoch 18/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4674 - acc: 0.8025 - val_loss: 0.4886 - val_acc: 0.7465\n",
      "Epoch 19/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4717 - acc: 0.7959 - val_loss: 0.4838 - val_acc: 0.7481\n",
      "Epoch 20/100\n",
      "2587/2587 [==============================] - 2s - loss: 0.4846 - acc: 0.7909 - val_loss: 0.4983 - val_acc: 0.7280\n",
      "Epoch 21/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4723 - acc: 0.7959 - val_loss: 0.4764 - val_acc: 0.7450\n",
      "Epoch 22/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4591 - acc: 0.7990 - val_loss: 0.4768 - val_acc: 0.7372\n",
      "Epoch 23/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4326 - acc: 0.8172 - val_loss: 0.4525 - val_acc: 0.7604\n",
      "Epoch 24/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4142 - acc: 0.8280 - val_loss: 0.4549 - val_acc: 0.7589\n",
      "Epoch 25/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4383 - acc: 0.8121 - val_loss: 0.4357 - val_acc: 0.7651\n",
      "Epoch 26/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4442 - acc: 0.8079 - val_loss: 0.4442 - val_acc: 0.7573\n",
      "Epoch 27/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4052 - acc: 0.8307 - val_loss: 0.4269 - val_acc: 0.7682\n",
      "Epoch 28/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4167 - acc: 0.8241 - val_loss: 0.4450 - val_acc: 0.7496\n",
      "Epoch 29/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4062 - acc: 0.8222 - val_loss: 0.4335 - val_acc: 0.7512\n",
      "Epoch 30/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.4089 - acc: 0.8326 - val_loss: 0.4352 - val_acc: 0.7496\n",
      "Epoch 31/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3783 - acc: 0.8415 - val_loss: 0.4301 - val_acc: 0.7543\n",
      "Epoch 32/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3748 - acc: 0.8353 - val_loss: 0.3996 - val_acc: 0.7728\n",
      "Epoch 33/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3586 - acc: 0.8508 - val_loss: 0.3811 - val_acc: 0.7852\n",
      "Epoch 34/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3762 - acc: 0.8373 - val_loss: 0.3778 - val_acc: 0.7867\n",
      "Epoch 35/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3615 - acc: 0.8496 - val_loss: 0.3902 - val_acc: 0.7774\n",
      "Epoch 36/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3467 - acc: 0.8469 - val_loss: 0.4105 - val_acc: 0.7759\n",
      "Epoch 37/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3507 - acc: 0.8547 - val_loss: 0.3928 - val_acc: 0.7805\n",
      "Epoch 38/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3473 - acc: 0.8531 - val_loss: 0.3556 - val_acc: 0.7975\n",
      "Epoch 39/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3482 - acc: 0.8508 - val_loss: 0.3650 - val_acc: 0.7898\n",
      "Epoch 40/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3477 - acc: 0.8612 - val_loss: 0.3589 - val_acc: 0.7960\n",
      "Epoch 41/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3334 - acc: 0.8612 - val_loss: 0.3682 - val_acc: 0.7852\n",
      "Epoch 42/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3353 - acc: 0.8620 - val_loss: 0.3633 - val_acc: 0.7883\n",
      "Epoch 43/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3123 - acc: 0.8709 - val_loss: 0.3581 - val_acc: 0.7944\n",
      "Epoch 44/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2913 - acc: 0.8852 - val_loss: 0.3592 - val_acc: 0.7944\n",
      "Epoch 45/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3060 - acc: 0.8817 - val_loss: 0.3381 - val_acc: 0.8114\n",
      "Epoch 46/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2972 - acc: 0.8836 - val_loss: 0.3356 - val_acc: 0.8114\n",
      "Epoch 47/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3082 - acc: 0.8821 - val_loss: 0.3289 - val_acc: 0.8192\n",
      "Epoch 48/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.3046 - acc: 0.8728 - val_loss: 0.3201 - val_acc: 0.8130\n",
      "Epoch 49/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2838 - acc: 0.8833 - val_loss: 0.3043 - val_acc: 0.8269\n",
      "Epoch 50/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2985 - acc: 0.8779 - val_loss: 0.3376 - val_acc: 0.8083\n",
      "Epoch 51/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2820 - acc: 0.8875 - val_loss: 0.3065 - val_acc: 0.8253\n",
      "Epoch 52/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2816 - acc: 0.8848 - val_loss: 0.2867 - val_acc: 0.8470\n",
      "Epoch 53/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2619 - acc: 0.8925 - val_loss: 0.2864 - val_acc: 0.8454\n",
      "Epoch 54/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2732 - acc: 0.8902 - val_loss: 0.2643 - val_acc: 0.8671\n",
      "Epoch 55/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2536 - acc: 0.8949 - val_loss: 0.2826 - val_acc: 0.8516\n",
      "Epoch 56/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2513 - acc: 0.9007 - val_loss: 0.2820 - val_acc: 0.8501\n",
      "Epoch 57/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2421 - acc: 0.9099 - val_loss: 0.2649 - val_acc: 0.8655\n",
      "Epoch 58/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2428 - acc: 0.9014 - val_loss: 0.2560 - val_acc: 0.8671\n",
      "Epoch 59/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2505 - acc: 0.9022 - val_loss: 0.2478 - val_acc: 0.8733\n",
      "Epoch 60/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2558 - acc: 0.9030 - val_loss: 0.2622 - val_acc: 0.8655\n",
      "Epoch 61/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2307 - acc: 0.9123 - val_loss: 0.2472 - val_acc: 0.8702\n",
      "Epoch 62/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2464 - acc: 0.9099 - val_loss: 0.2366 - val_acc: 0.8748\n",
      "Epoch 63/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2025 - acc: 0.9169 - val_loss: 0.2293 - val_acc: 0.8779\n",
      "Epoch 64/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2366 - acc: 0.9130 - val_loss: 0.2183 - val_acc: 0.8949\n",
      "Epoch 65/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2039 - acc: 0.9177 - val_loss: 0.2206 - val_acc: 0.8872\n",
      "Epoch 66/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2172 - acc: 0.9192 - val_loss: 0.2152 - val_acc: 0.8934\n",
      "Epoch 67/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2236 - acc: 0.9150 - val_loss: 0.2151 - val_acc: 0.8949\n",
      "Epoch 68/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2128 - acc: 0.9119 - val_loss: 0.2087 - val_acc: 0.8949\n",
      "Epoch 69/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2076 - acc: 0.9196 - val_loss: 0.1984 - val_acc: 0.9150\n",
      "Epoch 70/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.2010 - acc: 0.9215 - val_loss: 0.2080 - val_acc: 0.9088\n",
      "Epoch 71/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1798 - acc: 0.9300 - val_loss: 0.1796 - val_acc: 0.9181\n",
      "Epoch 72/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1922 - acc: 0.9293 - val_loss: 0.1737 - val_acc: 0.9243\n",
      "Epoch 73/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1761 - acc: 0.9382 - val_loss: 0.1687 - val_acc: 0.9289\n",
      "Epoch 74/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1975 - acc: 0.9289 - val_loss: 0.1502 - val_acc: 0.9490\n",
      "Epoch 75/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1907 - acc: 0.9285 - val_loss: 0.1754 - val_acc: 0.9274\n",
      "Epoch 76/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1892 - acc: 0.9219 - val_loss: 0.1465 - val_acc: 0.9521\n",
      "Epoch 77/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1754 - acc: 0.9304 - val_loss: 0.1423 - val_acc: 0.9552\n",
      "Epoch 78/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1737 - acc: 0.9347 - val_loss: 0.1517 - val_acc: 0.9521\n",
      "Epoch 79/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1700 - acc: 0.9382 - val_loss: 0.1375 - val_acc: 0.9567\n",
      "Epoch 80/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1681 - acc: 0.9354 - val_loss: 0.1252 - val_acc: 0.9614\n",
      "Epoch 81/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1650 - acc: 0.9335 - val_loss: 0.1343 - val_acc: 0.9552\n",
      "Epoch 82/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1534 - acc: 0.9420 - val_loss: 0.1220 - val_acc: 0.9691\n",
      "Epoch 83/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1556 - acc: 0.9424 - val_loss: 0.1192 - val_acc: 0.9691\n",
      "Epoch 84/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1510 - acc: 0.9416 - val_loss: 0.1311 - val_acc: 0.9567\n",
      "Epoch 85/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1546 - acc: 0.9393 - val_loss: 0.1289 - val_acc: 0.9614\n",
      "Epoch 86/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1550 - acc: 0.9409 - val_loss: 0.1225 - val_acc: 0.9660\n",
      "Epoch 87/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1517 - acc: 0.9463 - val_loss: 0.1136 - val_acc: 0.9691\n",
      "Epoch 88/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1377 - acc: 0.9505 - val_loss: 0.1016 - val_acc: 0.9722\n",
      "Epoch 89/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1410 - acc: 0.9470 - val_loss: 0.1051 - val_acc: 0.9691\n",
      "Epoch 90/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1532 - acc: 0.9467 - val_loss: 0.0973 - val_acc: 0.9753\n",
      "Epoch 91/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1401 - acc: 0.9525 - val_loss: 0.1204 - val_acc: 0.9629\n",
      "Epoch 92/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1454 - acc: 0.9440 - val_loss: 0.0941 - val_acc: 0.9737\n",
      "Epoch 93/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1366 - acc: 0.9482 - val_loss: 0.0877 - val_acc: 0.9753\n",
      "Epoch 94/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1209 - acc: 0.9586 - val_loss: 0.1072 - val_acc: 0.9691\n",
      "Epoch 95/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1313 - acc: 0.9501 - val_loss: 0.1021 - val_acc: 0.9706\n",
      "Epoch 96/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1261 - acc: 0.9525 - val_loss: 0.0947 - val_acc: 0.9722\n",
      "Epoch 97/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1236 - acc: 0.9583 - val_loss: 0.0988 - val_acc: 0.9722\n",
      "Epoch 98/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1170 - acc: 0.9555 - val_loss: 0.0855 - val_acc: 0.9768\n",
      "Epoch 99/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1306 - acc: 0.9544 - val_loss: 0.0893 - val_acc: 0.9737\n",
      "Epoch 100/100\n",
      "2587/2587 [==============================] - 1s - loss: 0.1306 - acc: 0.9552 - val_loss: 0.0756 - val_acc: 0.9784\n",
      "640/647 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, nb_epoch=100, batch_size=32, verbose=1, validation_data=(X_test, y_test))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HfDaF2yjc3zs",
    "outputId": "7af82f63-e3c2-4710-b47b-f457f8eae40d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "convolution2d_32 (Convolution2D) (None, 32, 126, 126)  320         convolution2d_input_14[0][0]     \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_32 (MaxPooling2D)   (None, 32, 63, 63)    0           convolution2d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_33 (Convolution2D) (None, 64, 61, 61)    18496       maxpooling2d_32[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_33 (MaxPooling2D)   (None, 64, 30, 30)    0           convolution2d_33[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution2d_34 (Convolution2D) (None, 128, 28, 28)   73856       maxpooling2d_33[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling2d_34 (MaxPooling2D)   (None, 128, 14, 14)   0           convolution2d_34[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_14 (Flatten)             (None, 25088)         0           maxpooling2d_34[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_41 (Dense)                 (None, 32)            802848      flatten_14[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_42 (Dense)                 (None, 64)            2112        dense_41[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_43 (Dense)                 (None, 128)           8320        dense_42[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_44 (Dense)                 (None, 3)             387         dense_43[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 906,339\n",
      "Trainable params: 906,339\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Defining the model \n",
    "# Feel free to use CNNs/Dense Networks\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(32, nb_row=3, nb_col=3, activation='relu', input_shape=X_train.shape[1:]))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(64, nb_row=3, nb_col=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Conv2D(128,nb_row=3, nb_col=3, activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2), strides=(2,2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(32, activation='sigmoid'))\n",
    "model.add(Dense(64, activation='sigmoid'))\n",
    "model.add(Dense(128, activation='sigmoid'))\n",
    "\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "# Viewing model_configuration\n",
    "model.summary()\n",
    "#model.get_config()\n",
    "#model.layers[0].get_config()\n",
    "#model.layers[0].input_shape\n",
    "#model.layers[0].output_shape\n",
    "#model.layers[0].output\n",
    "#model.layers[0].get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAp-otpbeS53"
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "learning_rate = 0.0001\n",
    "opt = Adam(lr=learning_rate)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=opt ,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eixqgy7MeVNT",
    "outputId": "aa7c1056-ebcf-4d54-faaf-adf4d5fca770"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2587 samples, validate on 647 samples\n",
      "Epoch 1/25\n",
      "2587/2587 [==============================] - 2s - loss: 1.0319 - acc: 0.5311 - val_loss: 0.9663 - val_acc: 0.5719\n",
      "Epoch 2/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.9353 - acc: 0.6332 - val_loss: 0.8801 - val_acc: 0.6692\n",
      "Epoch 3/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.8373 - acc: 0.6811 - val_loss: 0.7709 - val_acc: 0.7218\n",
      "Epoch 4/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.7266 - acc: 0.7228 - val_loss: 0.6557 - val_acc: 0.7481\n",
      "Epoch 5/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.6139 - acc: 0.7487 - val_loss: 0.5559 - val_acc: 0.7697\n",
      "Epoch 6/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.5240 - acc: 0.7596 - val_loss: 0.4777 - val_acc: 0.7743\n",
      "Epoch 7/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.4599 - acc: 0.8141 - val_loss: 0.4256 - val_acc: 0.9397\n",
      "Epoch 8/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.4059 - acc: 0.8713 - val_loss: 0.3745 - val_acc: 0.9366\n",
      "Epoch 9/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.3584 - acc: 0.9401 - val_loss: 0.3319 - val_acc: 0.9768\n",
      "Epoch 10/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.3168 - acc: 0.9772 - val_loss: 0.2953 - val_acc: 0.9815\n",
      "Epoch 11/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.2785 - acc: 0.9876 - val_loss: 0.2553 - val_acc: 0.9892\n",
      "Epoch 12/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.2379 - acc: 0.9930 - val_loss: 0.2206 - val_acc: 0.9830\n",
      "Epoch 13/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.2023 - acc: 0.9946 - val_loss: 0.1849 - val_acc: 0.9861\n",
      "Epoch 14/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.1682 - acc: 0.9973 - val_loss: 0.1536 - val_acc: 0.9938\n",
      "Epoch 15/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.1397 - acc: 0.9973 - val_loss: 0.1336 - val_acc: 0.9938\n",
      "Epoch 16/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.1199 - acc: 0.9969 - val_loss: 0.1140 - val_acc: 0.9938\n",
      "Epoch 17/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.1012 - acc: 0.9981 - val_loss: 0.0905 - val_acc: 0.9985\n",
      "Epoch 18/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0834 - acc: 0.9981 - val_loss: 0.0762 - val_acc: 0.9969\n",
      "Epoch 19/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0701 - acc: 0.9981 - val_loss: 0.0666 - val_acc: 0.9969\n",
      "Epoch 20/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0588 - acc: 0.9981 - val_loss: 0.0568 - val_acc: 0.9969\n",
      "Epoch 21/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0513 - acc: 0.9981 - val_loss: 0.0502 - val_acc: 0.9985\n",
      "Epoch 22/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0443 - acc: 0.9981 - val_loss: 0.0445 - val_acc: 0.9985\n",
      "Epoch 23/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0377 - acc: 0.9981 - val_loss: 0.0385 - val_acc: 0.9985\n",
      "Epoch 24/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0320 - acc: 0.9981 - val_loss: 0.0340 - val_acc: 0.9985\n",
      "Epoch 25/25\n",
      "2587/2587 [==============================] - 1s - loss: 0.0286 - acc: 0.9981 - val_loss: 0.0314 - val_acc: 0.9985\n",
      "640/647 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, y_train, nb_epoch=25, batch_size=32, verbose=1, validation_data=(X_test, y_test))\n",
    "results = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 379
    },
    "id": "JSwy3CV0er4a",
    "outputId": "99e4f0f7-5a9d-4904-f9f2-fb33fa651b74"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Accuracy on training set is 0.998067259373792\n",
      "\n",
      "Accuracy on test set is 0.9984544049459042\n"
     ]
    }
   ],
   "source": [
    "plt.plot(np.squeeze(history.history[\"loss\"]))\n",
    "plt.ylabel('cost')\n",
    "plt.xlabel('iterations (per tens)')\n",
    "plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"\\n\\nAccuracy on training set is {}\".format(history.history[\"acc\"][-1]))\n",
    "print(\"\\nAccuracy on test set is {}\".format(results[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZQ8enWDdhJxd"
   },
   "outputs": [],
   "source": [
    "# Checkpointer through Call-backs weights save on best model \n",
    "\n",
    "# model saving \n",
    "from keras.models import model_from_json\n",
    "from keras.models import load_model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uwQyDu8Yi-rq",
    "outputId": "a17287b7-91d7-4e3e-c059-ed50cbbb8266"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved model to disk\n"
     ]
    }
   ],
   "source": [
    "# serialize model to JSON\n",
    "model_json = model.to_json()\n",
    "with open(\"model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model.h5\")\n",
    "print(\"Saved model to disk\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IkL1Q1KJjAJF",
    "outputId": "4c633133-9537-467e-cfe1-77089f156dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "# load json and create model\n",
    "json_file = open('model.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"model.h5\")\n",
    "print(\"Loaded model from disk\")\n",
    "\n",
    "# Save the model in hdf5 file\n",
    "model.save('my_model')\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model= load_model('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "35hgsGoOjCEh"
   },
   "outputs": [],
   "source": [
    "loaded_model= load_model('my_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pAOK0iwtjD79",
    "outputId": "cb94673d-d257-4952-bce2-745188e68b0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647/647 [==============================] - 0s     \n",
      "Test Loss: 0.03142496463561998\n",
      "Test accuracy: 0.9984544049459042\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model w.r.t Test Loss and Test Accuracy\n",
    "score = model.evaluate(X_test, y_test)\n",
    "print('Test Loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kODCJOqCjFeF"
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 798
    },
    "id": "-KlguFk0jIBd",
    "outputId": "e1bb12ff-3d96-4809-9332-e205e0611787"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.01861342 0.97307056 0.00831595]\n",
      " [0.0118167  0.9824561  0.00572723]\n",
      " [0.00129707 0.01160904 0.98709387]\n",
      " ...\n",
      " [0.00355589 0.00273804 0.99370605]\n",
      " [0.06911533 0.9251504  0.00573424]\n",
      " [0.01040254 0.9825059  0.00709154]]\n",
      "640/647 [============================>.] - ETA: 0s[1 1 2 2 2 1 2 1 2 1 0 2 2 2 2 2 2 0 1 2 2 2 1 2 1 0 0 0 0 2 2 2 2 1 1 1 1\n",
      " 1 0 0 2 1 1 1 0 2 2 2 2 2 2 0 1 1 0 1 2 0 2 1 2 2 2 1 1 2 0 1 0 2 2 2 2 2\n",
      " 2 1 1 1 0 0 2 0 0 0 1 2 2 1 2 0 1 2 1 1 1 1 1 1 1 2 1 2 0 2 1 2 2 1 1 0 1\n",
      " 2 0 2 0 2 1 0 0 1 0 2 1 2 0 2 1 0 2 2 2 0 2 2 2 1 2 2 1 2 2 1 2 0 0 2 2 2\n",
      " 2 1 0 1 1 1 1 0 0 0 1 2 2 2 2 0 1 2 2 2 0 1 2 0 2 2 1 2 2 1 1 2 1 1 2 1 0\n",
      " 1 2 1 0 1 0 1 2 0 2 0 2 0 2 1 1 2 2 1 1 2 2 2 2 1 2 0 2 1 1 2 2 2 2 2 0 2\n",
      " 1 2 0 2 0 1 0 1 2 2 2 1 1 0 1 0 2 2 0 2 1 2 1 2 0 1 2 0 2 2 1 2 0 2 1 2 0\n",
      " 2 1 1 2 2 2 2 0 0 2 1 2 0 1 2 2 1 2 2 0 1 2 0 0 1 2 2 2 0 0 2 2 2 2 2 2 2\n",
      " 1 1 2 1 1 0 2 0 1 2 2 2 1 1 2 2 2 0 1 1 1 1 1 2 2 0 2 2 0 0 0 2 0 2 0 2 2\n",
      " 2 0 2 2 0 2 0 1 1 1 1 2 0 0 1 2 1 2 2 0 1 1 0 2 2 2 1 2 1 1 1 1 1 2 2 2 1\n",
      " 2 2 2 0 0 2 1 0 1 1 1 1 2 0 2 1 1 2 1 0 1 2 2 0 2 2 2 1 2 2 2 2 2 2 1 1 2\n",
      " 0 0 1 1 0 1 1 1 1 2 2 2 1 2 1 1 0 1 2 1 1 1 1 1 2 2 0 2 2 0 1 0 0 0 2 2 0\n",
      " 2 2 1 2 1 1 2 2 0 2 2 0 1 1 1 1 2 1 2 1 2 0 2 1 2 1 0 2 2 0 2 1 1 2 1 2 0\n",
      " 2 1 2 0 1 2 1 0 2 2 2 0 1 0 2 2 1 1 2 1 2 2 2 0 2 1 1 1 2 1 2 2 1 1 2 2 2\n",
      " 2 2 1 0 1 2 0 1 1 1 1 2 2 2 0 1 1 1 1 1 2 0 1 0 1 0 1 1 2 1 2 1 2 2 0 0 1\n",
      " 1 1 0 1 2 2 1 2 2 1 0 2 0 2 2 2 1 0 1 2 0 2 2 2 2 1 2 2 0 2 0 2 2 2 2 2 2\n",
      " 2 0 2 1 2 1 2 2 0 0 2 0 2 1 2 1 0 1 0 1 2 0 0 2 2 2 0 0 1 2 2 2 2 0 0 2 1\n",
      " 0 2 1 1 1 1 2 2 0 1 2 1 0 2 2 2 1 1]\n",
      "Confusion matrix, without normalization\n",
      "[[138   0   0]\n",
      " [  0 215   1]\n",
      " [  0   0 293]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predict model on Test Data (x_test)\n",
    "\n",
    "Y_pred = model.predict(X_test)\n",
    "print(Y_pred)\n",
    "\n",
    "# Printing the confusion matrix\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "import itertools\n",
    "\n",
    "# Print the classes of the Prediction\n",
    "y_pred = model.predict_classes(X_test)\n",
    "y_pred = np.array(y_pred)\n",
    "print(y_pred)\n",
    "\n",
    "target_names = ['AMD', 'DME', 'Normal']\n",
    "                                        \n",
    "# Plotting the confusion matrix\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "\n",
    "#Computation  confusion matrix\n",
    "cnf_matrix = (confusion_matrix(np.argmax(y_test,axis=1), y_pred)) # y_pred from model, y_tes = actuals\n",
    "\n",
    "np.set_printoptions(precision=3)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "# Plotting non-normalized confusion matrix\n",
    "plot_confusion_matrix(cnf_matrix, classes=target_names,\n",
    "                      title='Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jg0dPdwqjSHx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Untitled9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
